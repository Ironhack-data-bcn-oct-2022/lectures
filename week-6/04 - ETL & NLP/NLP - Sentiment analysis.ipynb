{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de4729f",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# <center> Natural Language Processing (NLP)</center>\n",
    "The [natural language processing](https://es.wikipedia.org/wiki/Procesamiento_de_natural_languages), abbreviated PLN3 —in English, natural language processing, NLP— is a field of sciences of computing, artificial intelligence and linguistics that studies the interactions between computers and human language. It deals with the formulation and investigation of computationally efficient mechanisms for communication between people and machines through natural language, that is, the world's languages. It is not about communication through natural languages ​​in an abstract way, but about designing mechanisms to communicate that are computationally efficient —that can be carried out by means of programs that execute or simulate communication—."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-questionnaire",
   "metadata": {},
   "source": [
    "![elgif](https://media.giphy.com/media/xT0xeJpnrWC4XWblEk/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab13503",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "NLP is considered one of the great challenges of artificial intelligence since it is one of the most complicated and challenging tasks: how to really understand the meaning of a text? How to undertand neologisms, ironies, jokes or poetry? If the strategy/algorithm we use does not overcome these difficulties, the results obtained will be of no use to us.\n",
    "In NLP it is not enough to understand mere words, you must understand the set of words that make up a sentence, and the set of lines that make up a paragraph. Giving a global meaning to the analysis of the text/discourse in order to draw good conclusions.\n",
    "\n",
    "Our language is full of ambiguities, of words with different meanings, twists and different meanings depending on the context. This makes NLP one of the most difficult tasks to master.\n",
    "\n",
    "Therefore, the difficulty of the NLP is at several levels:\n",
    "\n",
    "Ambiguity:\n",
    "\n",
    "- Lexical level: for example, several meanings\n",
    "- Referential level: anaphoras, metaphors, etc...\n",
    "- Structural level: semantics is necessary to understand the structure of a sentence\n",
    "- Pragmatic level: double meanings, irony, humor\n",
    "- Gaps detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-mission",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#The-data\" data-toc-modified-id=\"The-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>The data</a></span></li><li><span><a href=\"#We-bring-all-the-data-to-a-dataframe-from-MySQL\" data-toc-modified-id=\"We-bring-all-the-data-to-a-dataframe-from-MySQL-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>We bring all the data to a dataframe from MySQL</a></span></li><li><span><a href=\"#NLP\" data-toc-modified-id=\"NLP-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>NLP</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stop-Words\" data-toc-modified-id=\"Stop-Words-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Stop Words</a></span></li><li><span><a href=\"#Tokenize\" data-toc-modified-id=\"Tokenize-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Tokenize</a></span></li></ul></li><li><span><a href=\"#WordClouds\" data-toc-modified-id=\"WordClouds-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>WordClouds</a></span><ul class=\"toc-item\"><li><span><a href=\"#We-generate-a-WordCloud-of-a-song\" data-toc-modified-id=\"We-generate-a-WordCloud-of-a-song-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>We generate a WordCloud of a song</a></span></li><li><span><a href=\"#We-can-also-generate-it-from-a-column-of-an-entire-dataframe\" data-toc-modified-id=\"We-can-also-generate-it-from-a-column-of-an-entire-dataframe-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>We can also generate it from a column of an entire dataframe</a></span></li></ul></li><li><span><a href=\"#We-translate\" data-toc-modified-id=\"We-translate-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>We translate</a></span></li><li><span><a href=\"#Sentiment-analysis\" data-toc-modified-id=\"Sentiment-analysis-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Sentiment analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#TextBlob\" data-toc-modified-id=\"TextBlob-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>TextBlob</a></span></li><li><span><a href=\"#NLTK\" data-toc-modified-id=\"NLTK-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>NLTK</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install googletrans==4.0.0-rc1\\n\n",
    "#!pip install spacy\n",
    "#!pip install es-core-news-sm\n",
    "#!pip install nltk\n",
    "#!pip install wordcloud\n",
    "#!pip install langdetect\n",
    "#!pip install textblob\n",
    "#python -m spacy download en_core_web_lg\n",
    "#python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data management\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Databases\n",
    "import sqlalchemy as alch\n",
    "from getpass import getpass\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Languages\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import es_core_news_sm\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75156fe0",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## The data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1233a7c",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## We bring all the data to a dataframe from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-starter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "password = getpass(\"Introduce your password: \")\n",
    "dbName = \"spotify\"\n",
    "connectionData=f\"mysql+pymysql://root:{password}@localhost/{dbName}\"\n",
    "engine = alch.create_engine(connectionData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM newone\"\n",
    "df = pd.read_sql_query(query, engine)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0d0e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.iloc[3][\"lyrics\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69041abd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "as_it_was = df.iloc[3][\"lyrics\"]\n",
    "as_it_was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cf61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(as_it_was.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-shooting",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb8ce1",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Stop Words\n",
    "\n",
    "Empty words is the name given to words without meaning such as articles, pronouns, prepositions, etc. that are filtered before or after natural language data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9365cb",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Spacy library documentation\n",
    "https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = nlp.Defaults.stop_words\n",
    "#stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for element in as_it_was.split(\" \"):\n",
    "    if element not in stop:\n",
    "        new_list.append(element)\n",
    "string_without_stop = \" \".join(new_list)\n",
    "print(string_without_stop)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75deed13",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Tokenize\n",
    "One of the ways to normalize our tokens is through stemming and lemmatization.\n",
    "Stemming consists of removing and replacing suffixes from the root of the word. Lemmatization is a bit more complex and involves doing an analysis of the vocabulary and its morphology to return the basic form of the word (unconjugated, singular, etc).\n",
    "Read [this](https://medium.com/escueladeinteligenciaartificial/procesamiento-de-lenguaje-natural-stemming-y-lemmas-f5efd90dca8) interesting article.\n",
    "When it comes to tokenizing, we are going to do it by previously removing the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47010a1",
   "metadata": {},
   "source": [
    "![](https://d2mk45aasx86xg.cloudfront.net/difference_between_Stemming_and_lemmatization_8_11zon_452539721d.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d79251",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(string_without_stop)\n",
    "#tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa63bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = []\n",
    "for token in tokens:\n",
    "    lemmatized.append(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(\"what is this language\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df75d2",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "We are going to write a function that will tokenize the lyrics of our songs regardless of whether they are in Spanish or English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(txt):\n",
    "    try:\n",
    "        if detect(txt) == \"en\":\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "        elif detect(txt) == \"es\":\n",
    "            nlp = spacy.load(\"es_core_news_sm\")\n",
    "            \n",
    "        else:\n",
    "            return \"Not english nor spanish\"\n",
    "    except:\n",
    "        return \"Not able to analyze\"\n",
    "    \n",
    "    tokens = nlp(txt)\n",
    "    filtered = []\n",
    "    \n",
    "    \n",
    "    for token in tokens:\n",
    "        if not token.is_stop:\n",
    "            lemma = token.lemma_.lower().strip()\n",
    "            if re.search('^[a-zA-Z]+$',lemma): # This will remove the question marks\n",
    "                filtered.append(lemma)\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee45c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(\"diga'm-ho bé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37165218",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(\"no vestiu els nostres boscos de dol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(\"takk fyrir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(\"salam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(\"som-hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(\"hello how are you doing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cdbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"hello how are you doing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a24824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokenized\"] = df[\"lyrics\"].apply(tokenizer)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fed561",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "We check that it works by passing a letter to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3a6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenizer(df.loc[8][\"lyrics\"])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SO FAR: remove stop words\n",
    "# TOKENIZE THE STRINGS\n",
    "# WITH THE STRING TOKENIZED -> lemmatization\n",
    "# NEXT STEP: create lemmatization of the words: holding / holds / etc into hold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26c12b",
   "metadata": {},
   "source": [
    "# RECAP\n",
    "\n",
    "**FIRST PART**\n",
    "- I get the link for a spotify playlist\n",
    "- I sign up as a spotify developer\n",
    "- I get the token for authentication\n",
    "- I get the token for doing queries (i get the token by doing one request)\n",
    "- I get all the info from a playlist\n",
    "- I save songs, users into a dataframe\n",
    "\n",
    "- I use another API to get lyrics for every song\n",
    "\n",
    "**SECOND PART**\n",
    "- I crete a relational design for a database\n",
    "- I create check functions to filter those insert that already exist\n",
    "- I loop over the dataframe to try to insert those values in the tables\n",
    "- *pending debugging*\n",
    "\n",
    "**THIRD PART**\n",
    "- Clean the text: remove stop words\n",
    "- Tokenize: isolating the words\n",
    "- From the tokens: filter symbols\n",
    "- We can do lemmatization\n",
    "\n",
    "- THEN: df w/lyrics, & processed lyrics\n",
    "- Polarity and subjectivity of those texts\n",
    "\n",
    "Goal: ETL & a bit of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded8e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0][\"lyrics\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853741d3",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## WordClouds\n",
    "A word cloud or tag cloud is a visual representation of the words that make up a text, where the size is larger for the words that appear more frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-trance",
   "metadata": {},
   "source": [
    "![wordcloud](https://i.imgur.com/8I8aJ1N.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e799a9e",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### We generate a WordCloud of a song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c840fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = df.iloc[5][\"tokenized\"]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91142547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# having a string with no repeated words\n",
    "\n",
    "\" \".join(set(test.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=1600,height=400).generate(\" \".join(set(test.split(\" \"))))\n",
    "plt.figure(figsize=(15,10), facecolor=\"k\")\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "#plt.savefig('images/wordcloud.png', facecolor='k', bbox_inches='tight')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ba7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=1600,height=400).generate(counted)\n",
    "plt.figure(figsize=(15,10), facecolor=\"k\")\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "#plt.savefig('images/wordcloud.png', facecolor='k', bbox_inches='tight')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817bc549",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### We can also generate it from a column of an entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-polyester",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0efd5a24",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## We translate\n",
    "A little to our regret, although there are libraries that work in Spanish (the part of Spacy trained in Spanish works very well), the truth is that they work better in English, in general, there are other libraries that are not as exact and even so Spacy works best in English, so let's translate the lyrics.\n",
    "The TextBlob library, which we are going to use later to do sentiment analysis, also translates, but we are better going to use googletrans and its library, be careful when installing it:\n",
    "`pip install googletrans==3.1.0a0`\n",
    "You have to install the alpha version that the official one has issues.\n",
    "We create a column in the dataframe with all the translated letters, and leave the original as well, in case we need it.\n",
    "\n",
    "⚠️ PLEASE INSTALL THE LIBRARY AS IT SAYS ABOVE ⚠️ [stackoverflow](https://stackoverflow.com/questions/52455774/googletrans-stopped-working-with-error-nonetype-object-has-no-attribute-group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed8f5a",
   "metadata": {},
   "source": [
    "`pip install googletrans==4.0.0-rc1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425737fe",
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "# Let's see how to translate a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googletrans\n",
    "trans = googletrans.Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "esp = \"que tengas un buen day\"\n",
    "en = trans.translate(esp, dest=\"en\")\n",
    "en.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = \"no vestiu els nostres cosos de vermell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c092d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = trans.translate(cat, dest=\"en\")\n",
    "en.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "en.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the language something is written in: detect\n",
    "# we can use that to pass that google translate: google translate\n",
    "# we can have all the info in the same language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_into (string):\n",
    "    try:\n",
    "        trans = googletrans.Translator()\n",
    "        language = detect(string) # error handling\n",
    "        first = trans.translate(string, dest=\"en\")\n",
    "        return first.text\n",
    "    except:\n",
    "        string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_into(\"labas rytas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663dbbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokenized_en\"] = df[\"tokenized\"].apply(translate_into)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836b049",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Again we continue with the trend of automating and making functions for everything and thus be able to reuse code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece950f5",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Sentiment analysis\n",
    "### TextBlob\n",
    "`TextBlob(the_string).sentiment`\n",
    "\n",
    "**Arguments:** `string`<br>\n",
    "**Returns:** `polarity`& `subjectivity`\n",
    "\n",
    "\n",
    "The sentiment property returns a named tuple of the form Sentiment(polarity, subjectivity). The polarity score is a float in the range [-1.0, 1.0]. Subjectivity is a float in the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
    "\n",
    "TextBlob is supported by two libraries, NLTK and pattern, I leave you the [documentation](https://textblob.readthedocs.io/en/dev/)\n",
    "https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa7785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(\"This is the worst and it sucks\")\n",
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28644785",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46395f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ed54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68158f3f",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### NLTK\n",
    "The Natural Language Toolkit, or more commonly NLTK, is a set of symbolic and statistical natural language processing libraries and programs for the Python programming language. NLTK includes graphical demonstrations and sample data.\n",
    "\n",
    "In this case we will also get the polarity with the module [SentimentIntensityAnalizer](https://www.nltk.org/api/nltk.sentiment.html#module-nltk.sentiment.vader)\n",
    "\n",
    "`sia.polarity_scores(the_string)`\n",
    "\n",
    "**Aruments:** `string`<br>\n",
    "**Returns:** `polarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-strain",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nltk.downloader.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(df.iloc[5][\"tokenized_en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[5][\"tokenized_en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb54e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_sentence = \"the table is red\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01506eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(a_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a82c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sa (x):\n",
    "    try:\n",
    "        return sia.polarity_scores(x)\n",
    "    except:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c488b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa(\"You'd Be Crazy to Miss This Summer Super Sale ~2.50 Acres Rosamond, Kern County, CA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa(\"Your Wait Is Over, Rush Today For Sumer Sale!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d26dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa(\"This 2.50-acre parcel is nestled in Kern County, CA which is a great location from where you get easy access to everywhere. It is located just a few hours from Los Angeles. This is a good place for people who want to retreat from society and get away from it all to experience the ultimate relaxation that they’ve been dreaming of!Let the possibilities wash over you as you explore a stress-free and peaceful life at this quiet location in Mojave. It’s high time to allow yourself to get rid of the modern robotic life and embrace the brand-new simplified lifestyle. Don’t wait another decade to only wish you would have invested in this land!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e998ffc",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Information about the [compound](https://github.com/cjhutto/vaderSentiment#about-the-scoring). \n",
    "It is the sum of the scores normalized between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"] = df[\"tokenized_en\"].apply(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e37787",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530cb764",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.groupby([\"ironhacker\"])[\"sentiment\"].mean().sort_values().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed1cb64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.bar(df, x=\"ironhacker\", y=\"sentiment\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f0add",
   "metadata": {},
   "source": [
    "- NLP\n",
    "\n",
    "- Work with strings: regex\n",
    "- Python: split, replace, mpve things around\n",
    "\n",
    "- Tokenization: words\n",
    "- Lemmatization: roots of the words\n",
    "\n",
    "- Wordclouds: generate images and save them\n",
    "- Numeric values out of texts: compound & subjectivity \n",
    "\n",
    "- Group by, plot, I can see the sentiment analysis for some text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-a",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
